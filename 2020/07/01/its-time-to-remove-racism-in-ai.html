<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>It's time to remove racism in A.I.</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="stylesheet" href="/assets/css/style.css">
    <style type="text/tailwindcss">
        /* Custom Tailwind styles can go here if needed */
        /* For example, applying a default font if not using a theme that sets one */
        body {
            font-family: sans-serif; /* A generic sans-serif stack */
        }
        .prose { /* Basic prose styling for markdown content */
            max-width: 65ch;
        }
        .prose h2 { @apply text-2xl font-bold mb-2; }
        .prose p { @apply mb-4; }
        /* Add other prose styles as needed */
    </style>
    <link rel="icon" type="image/svg+xml" href="/assets/images/favicon.svg">
</head>
<body class="bg-white"> <!-- Default background, can be overridden by page content -->

    <!-- Removed fixed header and footer to allow page content to control full layout -->
    <style>
  /* Inherit fonts from homepage */
  @import url('https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@300;400;500;600;700&family=IBM+Plex+Mono:wght@400;600&display=swap');

  :root {
    --font-display: 'Space Grotesk', -apple-system, sans-serif;
    --font-mono: 'IBM Plex Mono', monospace;
    --accent: #DC2626;
  }

  .article-page {
    font-family: var(--font-display);
    background: #f5f5f5;
    min-height: 100vh;
  }

  .article-container {
    background: white;
    max-width: 900px;
    margin: 0 auto;
    padding: 4rem 5rem;
  }

  .mono {
    font-family: var(--font-mono);
  }

  /* Article Header */
  .article-header {
    position: relative;
    padding: 2rem 0 1.5rem;
  }

  .article-meta {
    display: flex;
    justify-content: space-between;
    align-items: center;
    font-family: var(--font-mono);
    font-size: 0.75rem;
    text-transform: uppercase;
    letter-spacing: 0.1em;
    color: #666;
    margin-bottom: 2rem;
  }

  .article-meta-info span {
    margin-right: 1.5rem;
  }

  .article-title {
    font-size: 3.5rem;
    font-weight: 700;
    line-height: 1.1;
    margin-bottom: 2rem;
    max-width: 900px;
  }

  .article-divider {
    width: 80px;
    height: 4px;
    background: var(--accent);
    margin: 1.5rem 0 0.75rem 0;
  }

  /* Article Content */
  .article-content {
    max-width: 720px;
    font-size: 1.125rem;
    line-height: 1.8;
    color: #1a1a1a;
  }

  .article-content p {
    margin-bottom: 1.5rem;
  }

  .article-content a {
    color: var(--accent);
    text-decoration: none;
    border-bottom: 2px solid var(--accent);
    transition: all 0.2s ease;
  }

  .article-content a:hover {
    background: var(--accent);
    color: white;
  }

  .article-content h2 {
    font-size: 2rem;
    font-weight: 700;
    margin-top: 3rem;
    margin-bottom: 1rem;
  }

  .article-content h3 {
    font-size: 1.5rem;
    font-weight: 600;
    margin-top: 2rem;
    margin-bottom: 1rem;
  }

  .article-content ul,
  .article-content ol {
    margin-bottom: 1.5rem;
    padding-left: 2rem;
  }

  .article-content li {
    margin-bottom: 0.5rem;
  }

  .article-content blockquote {
    border-left: 4px solid var(--accent);
    padding-left: 2rem;
    margin: 2rem 0;
    font-style: italic;
    color: #333;
  }

  .article-content code {
    font-family: var(--font-mono);
    background: #f5f5f5;
    padding: 0.2em 0.4em;
    font-size: 0.9em;
    border-radius: 2px;
  }

  .article-content pre {
    background: #1a1a1a;
    color: #f5f5f5;
    padding: 1.5rem;
    margin: 2rem 0;
    overflow-x: auto;
    font-family: var(--font-mono);
    font-size: 0.9rem;
  }

  .article-content img {
    max-width: 100%;
    height: auto;
    margin: 2rem 0;
  }

  /* Remove duplicate italic content that appears at start */
  .article-content > p:first-child em:only-child,
  .article-content > p:first-child > em:first-child:last-child {
    display: none;
  }

  /* Categories */
  .article-categories {
    display: flex;
    flex-wrap: wrap;
    gap: 0.75rem;
    margin-top: 3rem;
  }

  .category-tag {
    font-family: var(--font-mono);
    font-size: 0.7rem;
    text-transform: uppercase;
    letter-spacing: 0.05em;
    padding: 0.5rem 1rem;
    background: #000;
    color: white;
    clip-path: polygon(0 0, 95% 0, 100% 100%, 0 100%);
    transition: all 0.2s ease;
  }

  .category-tag:hover {
    background: var(--accent);
  }

  /* Article Navigation */
  .back-link {
    display: inline-flex;
    align-items: center;
    gap: 0.75rem;
    font-family: var(--font-mono);
    font-size: 0.875rem;
    text-transform: uppercase;
    letter-spacing: 0.1em;
    color: #000;
    text-decoration: none;
    padding: 1rem 2rem;
    border: 2px solid #000;
    transition: all 0.2s ease;
  }

  .back-link:hover {
    background: #000;
    color: white;
  }

  .back-link::before {
    content: '←';
    font-size: 1.2rem;
  }

  /* Article Footer */
  .article-footer {
    margin-top: 6rem;
    padding-top: 3rem;
    border-top: 2px solid #000;
  }

  /* Responsive */
  @media (max-width: 768px) {
    .article-container {
      padding: 2rem 1.5rem;
    }

    .article-title {
      font-size: 2.5rem;
    }

    .article-content {
      font-size: 1rem;
    }
  }
</style>

<article class="article-page">
  <div class="article-container">

      <!-- Header -->
      <header class="article-header">
        <div class="article-meta">
          <div class="article-meta-info">
            <span>July 01, 2020</span>
            
              <span>• Larry Velez</span>
            
          </div>
          <a href="/writing#year-2020" class="back-link">
            Back to Writing Archive
          </a>
        </div>

        <h1 class="article-title">It's time to remove racism in A.I.</h1>

        <div class="article-divider"></div>
      </header>

      <!-- Content -->
      <div class="article-content">
        <p>Over six months ago, well before the #BlackLivesMatter protests became widespread, I had written an article that highlighted an algorithm I discovered that clearly demonstrated the racial biases which are being baked into our technology. In that case, it was an algorithm that considered the population of Blacks in a neighborhood to help determine area property values. Now we discover that a data set used for facial recognition nearly put a man in jail erroneously.</p>

<p>Earlier this year, Robert Julian-Borchak Williams was handcuffed and arrested in front of his family for larceny. He was falsely identified by facial recognition technology, a tool regularly used by police.</p>

<p>According to The New York Times, when Williams was being interrogated in jail, investigators showed him a blurry picture of the suspect in question and he replied, “This is not me. You think all Black men look alike?”</p>

<p>Well apparently to facial recognition technology they do, and there is a reason.</p>

<p>“Recent studies by M.I.T. and the National Institute of Standards and Technology, or NIST, have found that while the technology works relatively well on white men, the results are less accurate for other demographics, in part because of a lack of diversity in the images used to develop the underlying databases,” explains the NYT.</p>

<p>The facial recognition software used by the Michigan State Police, DataWorks, uses components developed by two companies, NEC and Rank One Computing. “In 2019, algorithms from both companies were included in a federal study of over 100 facial recognition systems that found they were biased, falsely identifying African-American and Asian faces 10 times to 100 times more than Caucasian faces,” reports the NYT.</p>

<p>It is doubtful that Williams is the first Black person arrested and falsely accused due to a biased algorithm. As we rightly demand change in our society, A.I. cannot be overlooked because it has the outward appearance of being neutral. Understand that it is not the technology itself, but the data humans input to inform the A.I. results that are the problem. In this case, underrepresentation of Black men in facial recognition technology have biased the results.</p>

<p>The algorithms and data sets that drive technology are developed by humans who carry conscious and unconscious biases. As part of our effort to expunge racism in our society, we must identify and remove the bias data that are shaping dangerous, invisible decisions by A.I.</p>

      </div>

      <!-- Categories -->
      
        <div class="article-categories">
          
            <span class="category-tag">AI,</span>
          
            <span class="category-tag">technology,</span>
          
            <span class="category-tag">racial</span>
          
            <span class="category-tag">bias,</span>
          
            <span class="category-tag">facial</span>
          
            <span class="category-tag">recognition</span>
          
        </div>
      

  </div>
</article>


</body>
</html>
